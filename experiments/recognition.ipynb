{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "WORKING_DIR = os.path.dirname(os.path.abspath('./'))\n",
    "print(WORKING_DIR)\n",
    "sys.path.append(WORKING_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from csv file\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_csv('../dataset/queries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = df['Entities'].apply(json.loads)\n",
    "\n",
    "# group by dict keys of entities\n",
    "entitykeys = entities.dropna().explode()\n",
    "\n",
    "print(entitykeys.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_value = 'reaktionszeit'\n",
    "\n",
    "# Initialize an empty list to store the values\n",
    "bildschirmgroesse_values = []\n",
    "\n",
    "# Iterate over each item in the series\n",
    "for item in entities:\n",
    "    # Check if 'bildschirmgroesse' is a key in the dictionary\n",
    "    if current_value in item:\n",
    "        # Extend the list with the values of 'bildschirmgroesse' if value is not already in the list\n",
    "        bildschirmgroesse_values.extend(item[current_value])\n",
    "\n",
    "# Construct the JSON structure\n",
    "json_structure = {\n",
    "    \"type\": \"object\",\n",
    "    \"description\": f\"{current_value.capitalize()} des gewünschten Produkts.\",\n",
    "    \"properties\": {\n",
    "        \"values\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": f\"{current_value.capitalize()} die genannt wurden\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": list(set(bildschirmgroesse_values)),\n",
    "            },\n",
    "        },\n",
    "        \"notSpecified\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": f\"True wenn im Gespräch kein Wert für {current_value.capitalize()} nicht genannt wurde, sonst False\",\n",
    "        },\n",
    "        \"noSpecificUserPreference\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": f\"True wenn der Kunde keine bestimmte Präferenz für {current_value.capitalize()} hat, sonst False\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Print the JSON structure\n",
    "print(json.dumps(json_structure, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lorenz/Repos/study_entity_linking_new\n",
      "using google\n",
      "Preis <proto.marshal.collections.maps.MapComposite object at 0x7f8c133a4b80>\n",
      "Kategorie ['Laptop']\n",
      "Bildschirmgroesse ['15 Zoll', '16 Zoll']\n",
      "{'Preis': <proto.marshal.collections.maps.MapComposite object at 0x7f8c133a4b80>, 'Kategorie': ['Laptop'], 'Bildschirmgroesse': ['15 Zoll', '16 Zoll']}\n",
      "type <class 'proto.marshal.collections.maps.MapComposite'>\n",
      "convert to dict <proto.marshal.collections.maps.MapComposite object at 0x7f8c133a4b80>\n",
      "type <class 'proto.marshal.collections.repeated.RepeatedComposite'>\n",
      "type <class 'proto.marshal.collections.repeated.RepeatedComposite'>\n",
      "[FilterGeneratorOutput(id='Preis', values=[], minimum=None, maximum=1050), FilterGeneratorOutput(id='Kategorie', values=['Laptop'], minimum=None, maximum=None), FilterGeneratorOutput(id='Bildschirmgroesse', values=['15 Zoll', '16 Zoll'], minimum=None, maximum=None)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "WORKING_DIR = os.path.dirname(os.path.abspath('./'))\n",
    "print(WORKING_DIR)\n",
    "sys.path.append(WORKING_DIR)\n",
    "\n",
    "from app.entity_linking import EntityLinking\n",
    "from app.foundation_models.chat_openai import AIModelType\n",
    "from app.schema import json_schema\n",
    "\n",
    "\n",
    "async def recognize_filters(message: str):\n",
    "    # generate function call schema\n",
    "    schema = json_schema\n",
    "    \n",
    "    model = AIModelType.GOOGLE_GEMINI_PRO\n",
    "\n",
    "    # gemerate function calling schema\n",
    "    llm_module = EntityLinking(schema=schema,model=model)\n",
    "    \n",
    "    if model == AIModelType.MISTRAL_LARGE or model == AIModelType.MISTRAL_MIXTRAL_8x22B:\n",
    "        filter_generator_output = llm_module.generate_sync(conversation=message)\n",
    "    else:\n",
    "        filter_generator_output = await llm_module.generate_async(conversation=message)\n",
    "\n",
    "    # filter out empty filters\n",
    "    recognized_filters = [\n",
    "        filter\n",
    "        for filter in list(map(lambda x: x.dict(), filter_generator_output))\n",
    "        if len(filter[\"values\"]) > 0\n",
    "    ]\n",
    "\n",
    "    return (\n",
    "        filter_generator_output,\n",
    "        recognized_filters,\n",
    "    )\n",
    "    \n",
    "# test the function\n",
    "message = \"Ich suche einen Laptop mit 15 oder 16 zoll unter 1050 euro.\"\n",
    "filter_generator_output, recognized_filters = await recognize_filters(message)\n",
    "print(filter_generator_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from vertexai.generative_models import (\n",
    "    Content,\n",
    "    FunctionDeclaration,\n",
    "    GenerativeModel,\n",
    "    Part,\n",
    "    Tool,\n",
    ")\n",
    "\n",
    "\n",
    "model = GenerativeModel(\"gemini-1.5-pro-001\")\n",
    "\n",
    "get_exchange_rate_func = FunctionDeclaration(\n",
    "    name=\"get_exchange_rate\",\n",
    "    description=\"Get the exchange rate for currencies between countries\",\n",
    "    parameters={\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"currency_date\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A date that must always be in YYYY-MM-DD format or the value 'latest' if a time period is not specified\"\n",
    "        },\n",
    "        \"currency_from\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The currency to convert from in ISO 4217 format\"\n",
    "        },\n",
    "        \"currency_to\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The currency to convert to in ISO 4217 format\"\n",
    "        }\n",
    "    },\n",
    "         \"required\": [\n",
    "            \"currency_from\",\n",
    "            \"currency_date\",\n",
    "      ]\n",
    "  },\n",
    ")\n",
    "\n",
    "exchange_rate_tool = Tool(\n",
    "    function_declarations=[get_exchange_rate_func],\n",
    ")\n",
    "\n",
    "prompt = \"\"\"What is the exchange rate from Australian dollars to Swedish krona?\n",
    "How much is 500 Australian dollars worth in Swedish krona?\"\"\"\n",
    "\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    tools=[exchange_rate_tool],\n",
    ")\n",
    "\n",
    "response.candidates[0].content\n",
    "\n",
    "params = {}\n",
    "for key, value in response.candidates[0].content.parts[0].function_call.args.items():\n",
    "    params[key[9:]] = value\n",
    "params\n",
    "\n",
    "import requests\n",
    "url = f\"https://api.frankfurter.app/{params['date']}\"\n",
    "api_response = requests.get(url, params=params)\n",
    "api_response.text\n",
    "\n",
    "response = model.generate_content(\n",
    "    [\n",
    "    Content(role=\"user\", parts=[\n",
    "        Part.from_text(prompt + \"\"\"Give your answer in steps with lots of detail\n",
    "            and context, including the exchange rate and date.\"\"\"),\n",
    "    ]),\n",
    "    Content(role=\"function\", parts=[\n",
    "        Part.from_dict({\n",
    "            \"function_call\": {\n",
    "                \"name\": \"get_exchange_rate\",\n",
    "            }\n",
    "        })\n",
    "    ]),\n",
    "    Content(role=\"function\", parts=[\n",
    "        Part.from_function_response(\n",
    "            name=\"get_exchange_rate\",\n",
    "            response={\n",
    "                \"content\": api_response.text,\n",
    "            }\n",
    "        )\n",
    "    ]),\n",
    "    ],\n",
    "    tools=[exchange_rate_tool],\n",
    ")\n",
    "\n",
    "\n",
    "response.candidates[0].content.parts[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over rows with iterrows()\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "tasks = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    predicted_entities = row[7]\n",
    "\n",
    "    # if predicted_entities is not None and type(predicted_entities) == str:\n",
    "    #     print(\"Already predicted entities\")\n",
    "    #     continue\n",
    "\n",
    "    StateStore().clear()\n",
    "    query = row[1]\n",
    "    actual_category = row[2]\n",
    "    print(actual_category)\n",
    "    print(query)\n",
    "\n",
    "    async def recognize_and_return_index(\n",
    "        index, actual_category, filter_ids_list, message: str\n",
    "    ):\n",
    "        (filter_generator_output, products, recognized_filters) = (\n",
    "            await recognize_filters(\n",
    "                filter_ids_list=filter_ids_list,\n",
    "                category=actual_category,\n",
    "                message=message,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        df.at[index, \"Recognized_Filters_Prediction\"] = json.dumps(recognized_filters)\n",
    "        return index\n",
    "\n",
    "    # get filter for identified category\n",
    "    print(index)\n",
    "    if actual_category == \"Other\":\n",
    "        continue\n",
    "\n",
    "    # (filter_ids_list, _event) = await get_filter_ids(actual_category)\n",
    "    with open(\"filters_per_category.json\") as f:\n",
    "        filter_ids = json.load(f)\n",
    "        filter_ids = filter_ids[actual_category]\n",
    "        print(filter_ids)\n",
    "\n",
    "    await recognize_and_return_index(\n",
    "        index=index,\n",
    "        filter_ids_list=filter_ids,\n",
    "        actual_category=actual_category,\n",
    "        message=query,\n",
    "    )\n",
    "\n",
    "    # write to csv\n",
    "    df.to_csv(\"OTTO-QUERY-23-02-28-Entity_Recognition_test.csv\", index=False)\n",
    "\n",
    "\n",
    "print(len(tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write to csv\n",
    "# df.to_csv(\"OTTO-QUERY-23-02-28-Entity_Recognition_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    \"\"\"Normalize a string by removing spaces, converting to lowercase, and replacing dashes.\"\"\"\n",
    "    return s.replace(' ', '').replace('-', '').replace('/','').lower()\n",
    "\n",
    "mapping = {\n",
    "    \"ultrahd4k\": \"ultrahd\",\n",
    "    \"größerals84zoll\": 'gr\\\\u00f6\\\\u00dferals84zoll',\n",
    "    \"größerals19zoll\": 'gr\\\\u00f6\\\\u00dferals19zoll',\n",
    "    \"144hz\": \"144hz\",\n",
    "}\n",
    "\n",
    "speicherkapazitaet_mapping = {\n",
    "    \"256\" : \"256gb\",\n",
    "    \"512\" : \"512gb\",\n",
    "    \"1024\" : \"1024gb\",\n",
    "    \"128\" : \"128gb\",\n",
    "    \"64\" : \"64gb\",\n",
    "}\n",
    "\n",
    "bildwiederholungsfrequenz_mapping = {\n",
    "    \"60\" : \"60hz\",\n",
    "    \"120\" : \"120hz\",\n",
    "    \"144\" : \"144hz\",\n",
    "    \"165\": \"165hz\",\n",
    "    \"240\" : \"240hz\",\n",
    "}\n",
    "\n",
    "\n",
    "def split_entities_in_separated_entities(filter) -> list[tuple]:\n",
    "    separated_entities = []\n",
    "    for (key,values) in filter.items():\n",
    "        for value in values:\n",
    "            normalized_value = normalize_string(value)\n",
    "            mapped_value = mapping.get(normalized_value, normalized_value)\n",
    "            if key == \"speicherkapazitaet\":\n",
    "                mapped_value = speicherkapazitaet_mapping.get(normalized_value, normalized_value)\n",
    "            if key == \"bildwiederholungsfrequenz\":\n",
    "                mapped_value = bildwiederholungsfrequenz_mapping.get(normalized_value, normalized_value)\n",
    "            separated_entities.append((key, mapped_value))\n",
    "    return separated_entities\n",
    "\n",
    "def calculate_tp_fn_fp(actual: list[tuple], predicted: list[tuple]):\n",
    "    \"\"\"\n",
    "    Calculates the true positives (tp), false negatives (fn), and false positives (fp) \n",
    "    for a given set of actual and predicted entities.\n",
    "\n",
    "    Args:\n",
    "        actual (list[tuple]): The list of actual entities.\n",
    "        predicted (list[tuple]): The list of predicted entities.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the counts of tp, fn, fp, and tn.\n",
    "    \"\"\"\n",
    "\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "\n",
    "    fp_entities = []\n",
    "    fn_entities = []\n",
    "\n",
    "    if len(actual) == 0 and len(predicted) == 0:\n",
    "        tn = 1\n",
    "        return tp, fn, fp, tn, fp_entities,fn_entities\n",
    "\n",
    "\n",
    "    for entity in actual:\n",
    "        if entity in predicted:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "            fn_entities.append((actual, entity))\n",
    "\n",
    "    for entity in predicted:\n",
    "        if entity not in actual:\n",
    "            fp += 1\n",
    "            fp_entities.append((actual, entity))\n",
    "\n",
    "    return tp, fn, fp, tn, fp_entities, fn_entities\n",
    "\n",
    "\n",
    "def calculate_precision_recall_f1(tp, fn, fp):\n",
    "    if tp + fp > 0:\n",
    "        precision = tp / (tp + fp)\n",
    "    else:\n",
    "        precision = 0\n",
    "\n",
    "    if tp + fn > 0:\n",
    "        recall = tp / (tp + fn)\n",
    "    else:\n",
    "        recall = 0\n",
    "\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "total_tp = 0\n",
    "total_fn = 0\n",
    "total_fp = 0\n",
    "total_tn = 0\n",
    "\n",
    "false_positives = []\n",
    "false_negatives = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    query = row[1]\n",
    "    print(query)\n",
    "    print(row[3])\n",
    "    actual_entities = json.loads(row[3])\n",
    "    actual_category = row[2]\n",
    "\n",
    "    if actual_category == \"Other\":\n",
    "        continue\n",
    "\n",
    "    predicted_entities = row[7]\n",
    "\n",
    "    # check if predicted entities is nan\n",
    "    if isinstance(predicted_entities, float) and math.isnan(predicted_entities):\n",
    "        predicted_entities = []\n",
    "    else:\n",
    "        predicted_entities = json.loads(predicted_entities)\n",
    "\n",
    "    # convert predicted entities to dict\n",
    "    predicted_entities = {entity['id']: entity['values'] for entity in predicted_entities}\n",
    "\n",
    "\n",
    "    # process predicted entities\n",
    "    predicted_entities = split_entities_in_separated_entities(predicted_entities)\n",
    "    actual_entities = split_entities_in_separated_entities(actual_entities)\n",
    "\n",
    "    (tp, fn, fp, tn, fp_entities,fn_entities) = calculate_tp_fn_fp(actual_entities, predicted_entities)\n",
    "    total_tp += tp\n",
    "    total_fn += fn\n",
    "    total_fp += fp\n",
    "    total_tn += tn\n",
    "\n",
    "    if len(fp_entities) > 0:\n",
    "        false_positives.append((query, fp_entities))\n",
    "\n",
    "    if len(fn_entities) > 0:\n",
    "        false_negatives.append((query, fn_entities))\n",
    "\n",
    "\n",
    "# calculate precision, recall, and f1\n",
    "precision, recall, f1 = calculate_precision_recall_f1(total_tp, total_fn, total_fp)\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "print(\"total tn\", total_tn)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write false positives to file\n",
    "with open('false_positives.txt', 'w') as f:\n",
    "    for fp in false_positives:\n",
    "        f.write(f\"{fp}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write false negatives to file\n",
    "with open('false_negatives.txt', 'w') as f:\n",
    "    for fn in false_negatives:\n",
    "        f.write(f\"{fn}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the results in a confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# create confusion matrix\n",
    "conf_matrix = np.array([[total_tp, total_fn], [total_fp, total_tn]])\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['True', 'False'], yticklabels=['True', 'False'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have lots of false negatives, as the model does not have all filter ids in schema for Function Call Model. We use custom filter ids for the experiment, to give them a fair chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_entities(x):\n",
    "    if isinstance(x, float) and math.isnan(x):\n",
    "        x = []\n",
    "    else:\n",
    "        x = json.loads(x)\n",
    "\n",
    "    return {entity[\"id\"]: entity[\"values\"] for entity in x}\n",
    "\n",
    "\n",
    "# Detect all filters that are never recognized\n",
    "actual_entities = df[\"Entities\"].apply(json.loads).explode().dropna().unique()\n",
    "\n",
    "predicted_entities = df[\"Recognized_Filters_Prediction\"].apply(process_entities)\n",
    "predicted_entities = predicted_entities.explode().dropna().unique()\n",
    "\n",
    "never_recognized = [entity for entity in actual_entities if entity not in predicted_entities]\n",
    "print(never_recognized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"Category\")\n",
    "unique_entities = grouped['Entities'].apply(lambda x: x.apply(json.loads).explode().dropna().unique())\n",
    "    \n",
    "# store to json\n",
    "unique_entities.to_json('filters_per_category.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shopping-assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
